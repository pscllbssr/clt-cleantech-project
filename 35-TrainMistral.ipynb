{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-08T21:50:20.087100600Z",
     "start_time": "2024-06-08T21:50:20.031997800Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2594613434.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[1], line 6\u001B[1;36m\u001B[0m\n\u001B[1;33m    pip install accelerate -U\u001B[0m\n\u001B[1;37m        ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#!pip install torch transformers datasets\n",
    "#!pip install scipy==1.11.1         \n",
    "#!pip install mistral_inference\n",
    "#!pip install transformers\n",
    "#!pip install accelerate -U\n",
    "#!pip install transformers[torch]\n",
    "#!pip install accelerate -U    \n",
    "#!pip install markdown           \n",
    "#!pip install nltk          \n",
    "#!pip install more_itertools        \n",
    "#!pip install matplotlib             \n",
    "#Depending on your system, you might need different version, see https://pytorch.org/get-started/locally/\n",
    "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from mistral_inference.model import Transformer\n",
    "from mistral_inference.generate import generate\n",
    "\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "from mistral_common.protocol.instruct.messages import UserMessage\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "from huggingface_hub import HfApi, list_models"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-08T21:50:20.045067300Z"
    }
   },
   "id": "c6fb35c728d8f04e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#You might need to log in in the terminal via \"huggingface-cli login\" and enter token below\n",
    "hf_api = HfApi(\n",
    "    endpoint=\"https://huggingface.co\", # Can be a Private Hub endpoint.\n",
    "    token=\"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\", # Token is not persisted on the machine.\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-08T21:50:20.045067300Z"
    }
   },
   "id": "55f01d620022a3f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import requests\n",
    "from markdown import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "# Download the markdown file\n",
    "url = 'https://raw.githubusercontent.com/pscllbssr/clt-cleantech-project/main/faq_media.md'\n",
    "response = requests.get(url)\n",
    "md_content = response.text\n",
    "\n",
    "# Parse the markdown file\n",
    "qa_pairs = []\n",
    "lines = md_content.split('\\n')\n",
    "question, answer = None, None\n",
    "\n",
    "for line in lines:\n",
    "    if line.startswith('# Q:'):\n",
    "        if question and answer:\n",
    "            qa_pairs.append({'question': question, 'answer': answer})\n",
    "        question = line.replace('# Q:', '').strip()\n",
    "        answer = None\n",
    "    elif line.startswith('A:'):\n",
    "        answer = line.replace('A:', '').strip()\n",
    "\n",
    "# Add the last QA pair\n",
    "if question and answer:\n",
    "    qa_pairs.append({'question': question, 'answer': answer})\n",
    "\n",
    "# Save QA pairs to JSON file (optional)\n",
    "with open('qa_dataset.json', 'w') as f:\n",
    "    json.dump(qa_pairs, f, indent=2)\n",
    "\n",
    "# Load QA pairs into a Dataset\n",
    "dataset = Dataset.from_dict({'question': [qa['question'] for qa in qa_pairs],\n",
    "                             'answer': [qa['answer'] for qa in qa_pairs]})\n",
    "print(dataset[\"question\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-08T21:50:20.045067300Z"
    }
   },
   "id": "b5b9c9ce67c99abd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-v0.1')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Adding padding token\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [q for q in examples['question']]\n",
    "    targets = [a for a in examples['answer']]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding='max_length')\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=512, truncation=True, padding='max_length')\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-08T21:50:20.045067300Z"
    }
   },
   "id": "367415381a7c0d82"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(tokenized_dataset[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-08T21:50:20.045067300Z"
    }
   },
   "id": "583ce604dedda9c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(tokenized_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-08T21:50:20.045067300Z"
    }
   },
   "id": "4b49fd82dc0d6880"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "model = AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-v0.1')\n",
    "\n",
    "print(\"Before resizing:\")\n",
    "print(\"Model embedding size:\", model.get_input_embeddings().weight.size(0))\n",
    "print(\"Tokenizer vocabulary size:\", len(tokenizer))\n",
    "\n",
    "print(\"After resizing:\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(\"Model embedding size:\", model.get_input_embeddings().weight.size(0))\n",
    "print(\"Tokenizer vocabulary size:\", len(tokenizer))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-08T21:50:20.060700300Z"
    }
   },
   "id": "d0d14b5d2ac9d751"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "else:\n",
    "    print(\"No Cuda\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-08T21:50:20.060700300Z"
    }
   },
   "id": "ef998eda983b56f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=1,\n",
    "    save_steps=5,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_accumulation_steps=8,\n",
    "    fp16=True  # Enable mixed precision training\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-08T21:50:20.060700300Z"
    }
   },
   "id": "3b8bc6e518a555b4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-08T21:50:20.067713500Z"
    }
   },
   "id": "403c673535ad4e4f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
