{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training Mistral 1.7\n",
    "After some research, we have determined that Mistral 1.7 is an ideal model for training with question - answer pairs.\n",
    "Unfortunately we were not able to get it to run on our devices, due to lack of RAM."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3807b85fd05fcc0"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def setup():\n",
    "    !pip install torch transformers datasets\n",
    "    !pip install scipy==1.11.1         \n",
    "    !pip install mistral_inference\n",
    "    !pip install transformers\n",
    "    !pip install accelerate -U\n",
    "    !pip install transformers[torch]\n",
    "    !pip install accelerate -U    \n",
    "    !pip install markdown           \n",
    "    !pip install nltk          \n",
    "    !pip install more_itertools        \n",
    "    !pip install matplotlib             \n",
    "    #Depending on your system, you might need different version, see https://pytorch.org/get-started/locally/\n",
    "    !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "    \n",
    "#setup()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-09T12:15:01.251922800Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import torch\n",
    "from huggingface_hub import HfApi, list_models\n",
    "import requests\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-09T12:15:01.307786Z",
     "start_time": "2024-06-09T12:15:01.267520500Z"
    }
   },
   "id": "c6fb35c728d8f04e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Huggingface access\n",
    "Mistral 7B is run via Huggingface. To set up the connection, the following steps are required:\n",
    "\n",
    " -Register at https://huggingface.co/\n",
    " -Request access for Mistral-7B-v0.1 at https://huggingface.co/mistralai/Mistral-7B-v0.1\n",
    " -Create an access token with \"Write\" authorization at https://huggingface.co/settings/tokens\n",
    " -Insert the token below"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55e17b9b749d9838"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "hf_api = HfApi(\n",
    "    endpoint=\"https://huggingface.co\", # Can be a Private Hub endpoint.\n",
    "    token=\"your_token_here\", # Token is not persisted on the machine.\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-09T12:15:01.307786Z",
     "start_time": "2024-06-09T12:15:01.283175800Z"
    }
   },
   "id": "55f01d620022a3f9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing data\n",
    "The prepared question - answer pairs need to be transformed into a dictionary.\n",
    "Cleantech media and google patent data are kept separately to be later used as training and test data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "358fabc2cc2fd989"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the purpose of Array Technologies' agreement with POSCO?;;;Array Technologies has entered into a multi-year supply arrangement with steelmaker POSCO to diversify and strengthen its global supply chain, providing access to POSCO's proprietary PosMAC material â€“ an alloy-coated corrosion-resistant steel.\n",
      "What are the main components of a IoT device's controlling means?;;;The main components include the fixed rotation motor provided with in the bottom of the controlling means shell's inner wall, the fixed axis of rotation that passes through the top of the bearing and controlling means casing, and extends to the top of the controlling means casing.\n",
      "['What does \"Disclosed\" refer to?', 'Is the energy recycling method suitable for all types of buildings?', 'What factors affect the growth of plants in different environments?', 'What are the key features of the utility model?', 'What components are typically found on the front and rear sides of a water tank?', 'What is the purpose of arranging multiple hydraulic jacks between bearing plates?', 'What are the benefits of using an \"el\" insect killing box?', 'What is the focus of the model?', 'What is the purpose of the water feed pump in this system?', 'What are the benefits of implementing water eutrophication prevention methods?', 'How does the solar energy intelligence domestic garbage bin utilize indoor light?', 'How does the system ensure low-carbon environmental protection?', 'What is the main purpose of the invention described?', 'What is the purpose of the telescopic fixing feet and rollers on an fighting device?', 'What is the purpose of using a data storage cloud in equipment manufacturing?', 'What components does the multifunctional rechargeable solar engineering mobile lighting vehicle consist of?', 'What are the implications of reversing a battery connection?', 'Can I use a cell panel for shading in summer as well as temperature control during power generation?', 'How does the solar energy system generate electricity?', 'How does the wind generating set optimize its performance?', 'How can I minimize the effectiveness of a protective barrier?', 'How does the pump drainage system prevent sediment buildup in the cistern?', 'What is the goal of implementing a comprehensive energy system for an offshore wind farm?', 'How does the photovoltaic tile structure affect the performance of the solar panel assembly?', 'What is the purpose of the vertical shaft in the submarine tunnel?', 'What is the composition of a water lowering structure in a down-the-hole plane gate?', 'What is the purpose of the water collecting tank in the sewage collecting mechanism?', 'What are the characteristics of a manhole protection device?', 'What is the configuration of the display screen in this design?', 'How do I prevent my solar panels from receiving circuit damage when exposed to external forces outdoors?', 'What role does the site selection play in the planning of hydrogen production development?', 'What problem does the novel solar energy water heater solve?', 'What are the benefits of using a device for generating solar and wind energy?', 'What is disclosed in the description of a combined cycle unit inlet air temperature regulation and control system?', 'How to control the discharged water flow?', 'How does the safety cap prevent damage when performing measuring and paying-off work?', 'What does a utility model provide in relation to rural domestic sewage treatment plants?', 'How are relevant parameters of hydraulic engineering generated in a self-adaptive manner?', 'What is included in the fold (410-2) of a solar panel?', 'What is the purpose of the connections on the work box lateral wall?', 'How can agriculture reduce water waste and utilize solar energy?', 'How is waste heat converted into usable energy?', \"What are the main components of a IoT device's controlling means?\"]\n",
      "['The facility converts solar energy into green hydrogen through water electrolysis.', 'As of year-end 2021.', 'The development of energy storage capacity in Kentucky increases the potential for solar energy projects in the state.', 'Yes, according to the Korean government, private developers are expected to build solar plants on idle sites along highways with a combined capacity of 243 MW by 2025.', 'The final results showed that 273 PV projects with a combined capacity of 268.7MW were selected through the procurement exercise, which was the eleventh and final auction scheme for utility-scale solar in Japan, awarding fixed tariffs.', 'The final results show that PV projects with a combined capacity of 26.2 MW were selected in the procurement exercise, with power ratings above 250 kW.', 'The agreement involves developing 2 GW of pumped storage and 1 GW of solar energy projects.', 'Battery prices are highly volatile because they fluctuate in response to changes in the dynamic Chinese electric vehicle market.', 'Such a framework would enable transparent tracking and verification of PV panels throughout their life cycle.', 'It would enable transparent tracking and verification of PV panels throughout their life cycle.', 'The national level of electrification in Nigeria increased to 15.72% in 2020, with a significant disparity between rural areas and urban areas.', 'According to our research, one such example is JinkoSolar, a Chinese solar module manufacturer that has developed solutions utilizing ESG standards and claims it offers huge opportunities for the global energy transition.', 'The total amount of solar energy projects completed between CS Energy and CVE North America will be over 50 MW.', \"The guidance hinders the construction of solar energy projects in many urban areas, including Chicago, which could affect the Sustainability Hub's ability to place its trainees on local solar projects.\", 'Amazon has a global renewable energy project portfolio of 206 projects, including 71 utility-scale wind and solar projects and 135 solar rooftops on facilities and stores, which will generate 8.5 GW of electricity production capacity.', 'According to the source, there were 346,143 workers who spent all or part of their time on solar.', 'To date, SunCommon has worked with 75 farmers in Vermont and New York on solar energy projects.', 'By signing the lake lease agreement, projects have secured tenurial rights like environmental compliance certificates.', 'Most of the time, delays in solar energy projects are normally caused by the consolidation of properties and late issuances of land conversion permits.', \"Lightsource bp has secured a proxy generation power purchase agreement (PGPPA) for solar energy projects, in partnership with Nephila Climate, through Allianz Global Corporate & Specialty's Capital Solutions unit.\", 'Lightsource bp entered into a virtual power purchase agreement (VPPA) with Verizon Communications Inc.', 'Array Technologies has been awarded a contract for up to 4 GW of trackers from Primoris Services Corp., a provider of specialty contracting services.', 'Kenyon Energy provides permanent financing for solar energy projects.', 'Solar energy projects like Big River support economic development, promote energy independence, create jobs and generate local tax revenue.', \"Array Technologies has entered into a multi-year supply arrangement with steelmaker POSCO to diversify and strengthen its global supply chain, providing access to POSCO's proprietary PosMAC material â€“ an alloy-coated corrosion-resistant steel.\"]\n"
     ]
    }
   ],
   "source": [
    "# Download the markdown file\n",
    "url_media= 'https://raw.githubusercontent.com/pscllbssr/clt-cleantech-project/main/faq_media.md'\n",
    "url_patent='https://raw.githubusercontent.com/pscllbssr/clt-cleantech-project/main/faq_patent.md'\n",
    "\n",
    "def tokenize_qa(url,filename):\n",
    "    response = requests.get(url)\n",
    "    md_content = response.text\n",
    "    \n",
    "    # Parse the markdown file\n",
    "    qa_pairs = []\n",
    "    lines = md_content.split('\\n')\n",
    "    question, answer = None, None\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('# Q:'):\n",
    "            if question and answer:\n",
    "                qa_pairs.append({'question': question, 'answer': answer})\n",
    "            question = line.replace('# Q:', '').strip()\n",
    "            answer = None\n",
    "        elif line.startswith('A:'):\n",
    "            answer = line.replace('A:', '').strip()\n",
    "    \n",
    "    # Add the last QA pair\n",
    "    if question and answer:\n",
    "        qa_pairs.append({'question': question, 'answer': answer})\n",
    "        print(question+\";;;\"+answer)\n",
    "    \n",
    "    # Save QA pairs to JSON file\n",
    "    with open(filename+'.json', 'w') as f:\n",
    "        json.dump(qa_pairs, f, indent=2)\n",
    "    return Dataset.from_dict({'question': [qa['question'] for qa in qa_pairs],\n",
    "                                 'answer': [qa['answer'] for qa in qa_pairs]})\n",
    "\n",
    "dataset_media=tokenize_qa(url_media, 'media')\n",
    "dataset_patent=tokenize_qa(url_patent,'patent')\n",
    "print(dataset_patent[\"question\"])\n",
    "print(dataset_media['answer'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-09T12:15:01.826088Z",
     "start_time": "2024-06-09T12:15:01.307786Z"
    }
   },
   "id": "b5b9c9ce67c99abd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Encoding\n",
    "The prepared question - answer pairs now need to be tokenized according to Mistral's needs."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f01d29a325babce5"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/25 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c709a6e3e774bfa9a01c34701c2ea68"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pbrun\\anaconda3\\envs\\GLMPascal2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/43 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7a297fae99e34ad9959c1232036462ac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Set up tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-v0.1')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Adding padding token\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [q for q in examples['question']]\n",
    "    targets = [a for a in examples['answer']]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding='max_length')\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=512, truncation=True, padding='max_length')\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset_training = dataset_media.map(preprocess_function, batched=True)\n",
    "tokenized_dataset_test = dataset_patent.map(preprocess_function, batched=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-09T12:15:02.279693900Z",
     "start_time": "2024-06-09T12:15:01.826088Z"
    }
   },
   "id": "367415381a7c0d82"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What process does the facility use to convert solar energy into hydrogen?', 'answer': 'The facility converts solar energy into green hydrogen through water electrolysis.', 'input_ids': [32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 1, 1824, 1759, 1235, 272, 12671, 938, 298, 6603, 13024, 3408, 778, 15208, 2383, 28804], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 1, 415, 12671, 9105, 1074, 13024, 3408, 778, 5344, 15208, 2383, 1059, 2130, 22154, 346, 15412, 28723]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset_training[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-09T12:15:02.279693900Z",
     "start_time": "2024-06-09T12:15:02.264066400Z"
    }
   },
   "id": "583ce604dedda9c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup Model\n",
    "Now the Model needs to be imported. Due to a problem with padding size, embedding size needed to be adjusted (by 1)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f1dc324b7487568"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1c0dde15ed1a480cb59f0b5021d6e145"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before resizing:\n",
      "Model embedding size: 32000\n",
      "Tokenizer vocabulary size: 32001\n",
      "After resizing:\n",
      "Model embedding size: 32001\n",
      "Tokenizer vocabulary size: 32001\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-v0.1').to(device)\n",
    "\n",
    "#Had a problem with padding, this code solves it\n",
    "print(\"Before resizing:\")\n",
    "print(\"Model embedding size:\", model.get_input_embeddings().weight.size(0))\n",
    "print(\"Tokenizer vocabulary size:\", len(tokenizer))\n",
    "\n",
    "print(\"After resizing:\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(\"Model embedding size:\", model.get_input_embeddings().weight.size(0))\n",
    "print(\"Tokenizer vocabulary size:\", len(tokenizer))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-09T12:17:37.051399600Z",
     "start_time": "2024-06-09T12:15:02.279693900Z"
    }
   },
   "id": "d0d14b5d2ac9d751"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up Training\n",
    "Here the training parameters are defined. Currently all parameters are set to minimize RAM usage."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e592024785da3502"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pbrun\\anaconda3\\envs\\GLMPascal2\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=1,\n",
    "    save_steps=5,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_accumulation_steps=8,\n",
    "    fp16=True  # Enable mixed precision training\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset_training,\n",
    "    eval_dataset=tokenized_dataset_test\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-09T12:37:43.169402900Z",
     "start_time": "2024-06-09T12:37:43.085153300Z"
    }
   },
   "id": "3b8bc6e518a555b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "Now for the actual training. Unfortunately, this is as far as we got: Our jupiter Kernels crashed at every attempt of executing the following command and when executing it in Python proper, it crashed as well due to lack of RAM."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcc62ad76802343c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-09T12:37:49.279311800Z"
    }
   },
   "id": "403c673535ad4e4f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
